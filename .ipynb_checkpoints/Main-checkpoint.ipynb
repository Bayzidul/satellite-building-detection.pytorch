{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residential Building Classification\n",
    "This is a bare-bone version of the residential vs. non-residential building classification. We use DenseNet in this implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from cs231n.data_utils import *\n",
    "import params\n",
    "\n",
    "# Load parameters\n",
    "params = vars(params)\n",
    "class_names = params['class_names']\n",
    "dtype = params['dtype']\n",
    "\n",
    "torch.set_num_threads(params['num_threads'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess FMOW Dataset\n",
    "Check if fMoW dataset is available. If so, process the images by cropping based on given bounding boxes. Load in traing, validation and test dataset along with ground truth labels for training and validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756373697bbe4f87b95f05c864d0dd3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=46), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting images from lake_or_pond...\n",
      "Extracting images from educational_institution...\n",
      "Extracting images from parking_lot_or_garage...\n",
      "Extracting images from military_facility...\n",
      "Extracting images from runway...\n",
      "Extracting images from tower...\n",
      "Extracting images from zoo...\n",
      "Extracting images from aquaculture...\n",
      "Extracting images from barn...\n",
      "Extracting images from border_checkpoint...\n",
      "Extracting images from dam...\n",
      "Extracting images from tunnel_opening...\n",
      "Extracting images from recreational_facility...\n",
      "Extracting images from hospital...\n",
      "Extracting images from police_station...\n",
      "Extracting images from electric_substation...\n",
      "Extracting images from railway_bridge...\n",
      "Extracting images from fire_station...\n",
      "Extracting images from swimming_pool...\n",
      "Extracting images from lighthouse...\n",
      "Extracting images from single-unit_residential...\n",
      "Extracting images from waste_disposal...\n",
      "Extracting images from airport_hangar...\n",
      "Extracting images from road_bridge...\n",
      "Extracting images from toll_booth...\n",
      "Extracting images from car_dealership...\n",
      "Extracting images from office_building...\n",
      "Extracting images from surface_mine...\n",
      "Extracting images from crop_field...\n",
      "Extracting images from fountain...\n",
      "Extracting images from solar_farm...\n",
      "Extracting images from prison...\n",
      "Extracting images from ground_transportation_station...\n",
      "Extracting images from factory_or_powerplant...\n",
      "Extracting images from wind_farm...\n",
      "Extracting images from storage_tank...\n",
      "Extracting images from golf_course...\n",
      "Extracting images from construction_site...\n",
      "Extracting images from multi-unit_residential...\n",
      "Extracting images from place_of_worship...\n",
      "Extracting images from race_track...\n",
      "Extracting images from smokestack...\n",
      "\n",
      "X_train <class 'numpy.ndarray'> (45, 1, 200, 200) float64\n",
      "y_train <class 'numpy.ndarray'> (45,) float64\n"
     ]
    }
   ],
   "source": [
    "data = load_mini_fmow(params, batch_size=50)\n",
    "\n",
    "# Unpack training and validation dat\n",
    "X_train = torch.from_numpy(data['X_train'])\n",
    "y_train = torch.from_numpy(data['y_train'])\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "loader_train = DataLoader(train_data, batch_size=64)\n",
    "\n",
    "# X_val = torch.from_numpy(data['X_val'])\n",
    "# y_val = torch.from_numpy(data['y_val'])\n",
    "# val_data = data_utils.TensorDataset(X_val, y_val)\n",
    "# loader_val = DataLoader(val_data, batch_size=64)\n",
    "\n",
    "# # Unpack test data\n",
    "# X_test = torch.from_numpy(data['X_test'])\n",
    "# y_test = torch.from_numpy(data['y_test'])\n",
    "# test_data = data_utils.TensorDataset(X_test, y_test)\n",
    "# loader_test = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# Print out all the keys and values from the data dictionary\n",
    "for k, v in data.items():\n",
    "    print(k, type(v), v.shape, v.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a small batch of data\n",
    "Plot a small batch of training images to get a sense of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2bc121c87f0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmini_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmini_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Sample a minibatch and show the images and captions\n",
    "# mini_size = 5\n",
    "#for i, y_i in enumerate(data['y_train'][:mini_size]):\n",
    "#    plt.subplot(1, batch_size, i+1)\n",
    "#    plt.imshow(data['X_train'][i, :, :, :])\n",
    "#    plt.axis('off')\n",
    "#    plt.title(class_names[y_mini[i]])\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "Load in pre-trained DenseNet. Configure training parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive two-layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model (3 conv layer)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=6, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(12*12*32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "#     200*200  \n",
    "#     98*98\n",
    "#     49*49\n",
    "#     24*24\n",
    "#     12*12\n",
    "#     11111---1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "batch_size = params['batch_size']\n",
    "num_epochs = params['num_epochs']\n",
    "learning_rate = params['learning_rate']\n",
    "print_every = 100\n",
    "\n",
    "model = CNN().type(dtype)\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "Train the model one epoch at a time. Keep the verbose option on to monitor the loss during training. \n",
    "Save training and validation accuracy history for visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Train function\n",
    "def train(model, loss_fn, optimizer, loader, epochs=1):\n",
    "    for epoch in range(epochs):\n",
    "        # Set on Training mode\n",
    "        model.train()\n",
    "        for t, (x, y) in enumerate(loader):\n",
    "            x_var = Variable(x.type(dtype))\n",
    "            y_var = Variable(y.type(dtype))\n",
    "            socres = model(x_var)\n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "def check_accuracy(model, loader):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for x, y in loader:\n",
    "        x_var = Variable(x.type(dtype), volatile=True)\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train(model, loss_fn, optimizer, loader_train, epochs=num_epochs)\n",
    "# Check accuracy of model\n",
    "check_accuracy(model, loader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Visualize training and validation accuracy throughout training history. \n",
    "Visualize loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "Make predictions based on test dataset. Visually assess the quality of prediction and compute accuracy based on groundtruth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy of model on Test set\n",
    "check_accuracy(model, loader_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
