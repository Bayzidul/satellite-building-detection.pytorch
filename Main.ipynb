{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residential Building Classification\n",
    "This is a bare-bone version of the residential vs. non-residential building classification. We use DenseNet in this implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from cs231n.data_utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess FMOW Dataset\n",
    "Check if fMoW dataset is available. If so, process the images by cropping based on given bounding boxes. Load in traing, validation and test dataset along with ground truth labels for training and validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['false_detection','residential','non_residential']\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "data = load_fmow('~/ext/data/fmow-rgb', dtype=dtype)\n",
    "# Unpack training and validation dat\n",
    "X_train = torch.from_numpy(data['X_train'])\n",
    "y_train = torch.from_numpy(data['y_train'])\n",
    "train_data = data_utils.TensorDataset(X_train, y_train)\n",
    "loader_train = DataLoader(train_loader, batch_size=64)\n",
    "\n",
    "X_val = torch.from_numpy(data['X_val'])\n",
    "y_val = torch.from_numpy(data['y_val'])\n",
    "val_data = data_utils.TensorDataset(X_val, y_val)\n",
    "loader_val = DataLoader(val_data, batch_size=64)\n",
    "\n",
    "# Unpack test data\n",
    "X_test = torch.from_numpy(data['X_test'])\n",
    "y_test = torch.from_numpy(data['y_test'])\n",
    "test_data = data_utils.TensorDataset(X_test, y_test)\n",
    "loader_test = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# Print out all the keys and values from the data dictionary\n",
    "for k, v in data.items():\n",
    "    print(k, type(v), v.shape, v.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a small batch of data\n",
    "Plot a small batch of training images to get a sense of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a minibatch and show the images and captions\n",
    "batch_size = 5\n",
    "\n",
    "X_mini, y_mini = sample_mini_batch(data['X_train'], data['y_train'], batch_size=batch_size)\n",
    "for i, y_i in enumerate(y_mini):\n",
    "    plt.subplot(1, batch_size, i+1)\n",
    "    plt.imshow(X_mini[i, :, :, :])\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_mini[i]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "Load in pre-trained DenseNet. Configure training parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive two-layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model (3 conv layer)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=6, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(12*12*32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "#     200*200  \n",
    "#     98*98\n",
    "#     49*49\n",
    "#     24*24\n",
    "#     12*12\n",
    "#     11111---1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 50\n",
    "print_every = 100\n",
    "\n",
    "model = CNN().type(dtype)\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "Train the model one epoch at a time. Keep the verbose option on to monitor the loss during training. \n",
    "Save training and validation accuracy history for visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Train function\n",
    "def train(model, loss_fn, optimizer, loader, epochs=1):\n",
    "    for epoch in range(epochs):\n",
    "        # Set on Training mode\n",
    "        model.train()\n",
    "        for t, (x, y) in enumerate(loader):\n",
    "            x_var = Variable(x.type(dtype))\n",
    "            y_var = Variable(y.type(dtype))\n",
    "            socres = model(x_var)\n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "def check_accuracy(model, loader):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for x, y in loader:\n",
    "        x_var = Variable(x.type(dtype), volatile=True)\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train(model, loss_fn, optimizer, loader_train, num_epochs=num_epochs)\n",
    "# Check accuracy of model\n",
    "check_accuracy(model, loader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Visualize training and validation accuracy throughout training history. \n",
    "Visualize loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "Make predictions based on test dataset. Visually assess the quality of prediction and compute accuracy based on groundtruth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy of model on Test set\n",
    "check_accuracy(model, loader_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
